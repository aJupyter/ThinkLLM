{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "我们将使用最简单的 Python 和 NumPy 库来复现 Transformer 的核心组件算法。\n",
        "\n",
        "这里我们不涉及模型的训练、batching 的优化或复杂的框架特性，只关注每个组件的数学运算和逻辑。\n",
        "\n",
        "我们将主要实现以下组件：\n",
        "\n",
        "1.  **Positional Encoding (位置编码):** 给序列中的每个位置提供一个唯一的向量表示。\n",
        "2.  **Scaled Dot-Product Attention (缩放点积注意力):** 注意力的核心计算单元。\n",
        "3.  **Multi-Head Attention (多头注意力):** 并行运行多个注意力机制，然后合并结果。\n",
        "4.  **Feed-Forward Network (前馈网络):** 一个简单的两层全连接网络，带激活函数。\n",
        "5.  **Add & Norm (残差连接与层归一化):** 残差连接防止梯度消失，层归一化稳定训练。\n",
        "6.  **Encoder Layer (编码器层):** 包含多头注意力和前馈网络，以及残差连接和层归一化。\n",
        "\n",
        "我们将使用 NumPy 数组来表示张量。形状约定通常是 `(batch_size, sequence_length, model_dim)` 或类似的。\n",
        "\n",
        "**准备工作:**"
      ],
      "metadata": {
        "id": "fcN1swsm8P_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# 为了简化，我们手动实现 softmax\n",
        "def softmax(x, axis=-1):\n",
        "    # 提高数值稳定性\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "fa5IYDjG8P_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Positional Encoding (位置编码)**\n",
        "\n",
        "通过正弦和余弦函数为序列中的每个位置生成一个固定向量。\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
        "\n",
        "其中，$pos$ 是位置，$i$ 是维度。"
      ],
      "metadata": {
        "id": "5ahpboMp8P_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    生成位置编码矩阵。\n",
        "\n",
        "    Args:\n",
        "        position: 最大序列长度。\n",
        "        d_model: 模型的维度。\n",
        "\n",
        "    Returns:\n",
        "        一个形状为 (1, position, d_model) 的 NumPy 数组。\n",
        "    \"\"\"\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n",
        "\n",
        "    # 对偶数索引应用 sin\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # 对奇数索引应用 cos\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...] # 添加 batch 维度\n",
        "    return pos_encoding.astype(np.float32)\n",
        "\n",
        "# --- Test Positional Encoding ---\n",
        "print(\"--- Test Positional Encoding ---\")\n",
        "max_seq_len = 50\n",
        "model_dim = 512\n",
        "pe = positional_encoding(max_seq_len, model_dim)\n",
        "print(f\"Positional Encoding shape: {pe.shape}\") # Expected: (1, 50, 512)\n",
        "print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Positional Encoding ---\n",
            "Positional Encoding shape: (1, 50, 512)\n",
            "--------------------\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPBNZysN8P_o",
        "outputId": "4bf387d4-bd36-4b1c-f3c1-ceda550bb40b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Scaled Dot-Product Attention (缩放点积注意力)**\n",
        "\n",
        "这是注意力的核心计算：计算 Query 和 Key 的相似度，缩放，通过 softmax 得到注意力权重，然后加权 Value。\n",
        "\n",
        "$$Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "其中 $d_k$ 是 Key 向量的维度。"
      ],
      "metadata": {
        "id": "fD4NK3EA8P_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    计算缩放点积注意力。\n",
        "\n",
        "    Args:\n",
        "        q: Query 张量，形状 (..., seq_len_q, depth)。\n",
        "        k: Key 张量，形状 (..., seq_len_k, depth)。\n",
        "        v: Value 张量，形状 (..., seq_len_v, depth_v)。\n",
        "            seq_len_k 必须等于 seq_len_v。\n",
        "        mask: Mask 张量 (用于屏蔽某些连接)，形状 (..., seq_len_q, seq_len_k)。\n",
        "              通常在解码器中使用 (Look-ahead mask) 或处理变长序列 (Padding mask)。\n",
        "\n",
        "    Returns:\n",
        "        output: 注意力计算结果，形状 (..., seq_len_q, depth_v)。\n",
        "        attention_weights: 注意力权重，形状 (..., seq_len_q, seq_len_k)。\n",
        "    \"\"\"\n",
        "    # 计算 Q 和 K 的点积 (相似度)\n",
        "    # 形状 (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = np.matmul(q, k.transpose(0, 1, 3, 2))\n",
        "\n",
        "    # 缩放\n",
        "    d_k = q.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(d_k)\n",
        "\n",
        "    # 应用 mask (如果提供了)\n",
        "    if mask is not None:\n",
        "        # 将 mask 为 0 的位置 (即需要忽略的位置) 的 logits 设置为一个非常小的负数\n",
        "        scaled_attention_logits = scaled_attention_logits + (mask * -1e9) # 使用 -1e9 代替负无穷\n",
        "\n",
        "    # softmax 得到注意力权重\n",
        "    # 形状 (..., seq_len_q, seq_len_k)\n",
        "    attention_weights = softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # 将权重应用于 V\n",
        "    # 形状 (..., seq_len_q, depth_v)\n",
        "    output = np.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# --- Test Scaled Dot-Product Attention ---\n",
        "print(\"--- Test Scaled Dot-Product Attention ---\")\n",
        "# 模拟输入 (batch=1, seq_len=3, depth=2)\n",
        "q = np.random.rand(1, 3, 2)\n",
        "k = np.random.rand(1, 3, 2)\n",
        "v = np.random.rand(1, 3, 2)\n",
        "mask = None # No mask for simplicity\n",
        "\n",
        "attention_output, attention_weights = scaled_dot_product_attention(q[np.newaxis, ...], # Add head dim for test\n",
        "                                      k[np.newaxis, ...], # Add head dim for test\n",
        "                                      v[np.newaxis, ...], # Add head dim for test\n",
        "                                      mask)\n",
        "# Remove the added head dim for printing\n",
        "attention_output = attention_output.squeeze(0)\n",
        "attention_weights = attention_weights.squeeze(0)\n",
        "\n",
        "print(f\"Q shape: {q.shape}\")\n",
        "print(f\"K shape: {k.shape}\")\n",
        "print(f\"V shape: {v.shape}\")\n",
        "print(f\"Attention Output shape: {attention_output.shape}\") # Expected: (1, 3, 2)\n",
        "print(f\"Attention Weights shape: {attention_weights.shape}\") # Expected: (1, 3, 3)\n",
        "print(\"Sample Attention Weights:\\n\", attention_weights[0]) # Print weights for the first item in batch\n",
        "print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Scaled Dot-Product Attention ---\n",
            "Q shape: (1, 3, 2)\n",
            "K shape: (1, 3, 2)\n",
            "V shape: (1, 3, 2)\n",
            "Attention Output shape: (1, 3, 2)\n",
            "Attention Weights shape: (1, 3, 3)\n",
            "Sample Attention Weights:\n",
            " [[0.25414612 0.39655761 0.34929627]\n",
            " [0.29750412 0.36078136 0.34171453]\n",
            " [0.24224829 0.40862682 0.34912489]]\n",
            "--------------------\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBImA_fJ8P_u",
        "outputId": "9bc6f07c-d9dd-46d5-b3fe-f1e64515a147"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Multi-Head Attention (多头注意力)**\n",
        "\n",
        "将 Query, Key, Value 分别通过线性层映射到 `num_heads` 个 \"头\"，每个头独立计算缩放点积注意力，然后将所有头的输出拼接起来，再通过一个线性层进行最终映射。"
      ],
      "metadata": {
        "id": "KzOI5jTb8P_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_heads(x, num_heads, depth):\n",
        "    \"\"\"\n",
        "    将最后一个维度分割成 (num_heads, depth)。\n",
        "    转置结果以便进行 attention 计算。\n",
        "\n",
        "    Args:\n",
        "        x: 输入张量，形状 (batch_size, seq_len, d_model)。\n",
        "        num_heads: 头数。\n",
        "        depth: 每个头的维度 (d_model / num_heads)。\n",
        "\n",
        "    Returns:\n",
        "        一个形状为 (batch_size, num_heads, seq_len, depth) 的张量。\n",
        "    \"\"\"\n",
        "    # 形状 (batch_size, seq_len, num_heads, depth)\n",
        "    x = x.reshape(x.shape[0], x.shape[1], num_heads, depth)\n",
        "    # 形状 (batch_size, num_heads, seq_len, depth)\n",
        "    return x.transpose(0, 2, 1, 3)\n",
        "\n",
        "def combine_heads(x):\n",
        "    \"\"\"\n",
        "    合并多头注意力的输出。\n",
        "\n",
        "    Args:\n",
        "        x: 输入张量，形状 (batch_size, num_heads, seq_len, depth)。\n",
        "\n",
        "    Returns:\n",
        "        一个形状为 (batch_size, seq_len, d_model) 的张量。\n",
        "    \"\"\"\n",
        "    # 形状 (batch_size, seq_len, num_heads, depth)\n",
        "    x = x.transpose(0, 2, 1, 3)\n",
        "    # 形状 (batch_size, seq_len, d_model)\n",
        "    d_model = x.shape[2] * x.shape[3]\n",
        "    return x.reshape(x.shape[0], x.shape[1], d_model)\n",
        "\n",
        "\n",
        "def multi_head_attention(q, k, v, mask, d_model, num_heads,\n",
        "                         Wq, Wk, Wv, Wo):\n",
        "    \"\"\"\n",
        "    实现多头注意力机制。\n",
        "\n",
        "    Args:\n",
        "        q: Query 张量，形状 (batch_size, seq_len_q, d_model)。\n",
        "        k: Key 张量，形状 (batch_size, seq_len_k, d_model)。\n",
        "        v: Value 张量，形状 (batch_size, seq_len_v, d_model)。\n",
        "            seq_len_k 必须等于 seq_len_v。\n",
        "        mask: Mask 张量，形状 (batch_size, 1, 1, seq_len_k) 或 (batch_size, 1, seq_len_q, seq_len_k)。\n",
        "        d_model: 模型的维度。\n",
        "        num_heads: 头数。\n",
        "        Wq, Wk, Wv, Wo: 分别是 Q, K, V 投影和最终输出投影的权重矩阵。\n",
        "                      Wq, Wk, Wv 形状 (d_model, d_model)。 Wo 形状 (d_model, d_model)。\n",
        "\n",
        "    Returns:\n",
        "        output: 多头注意力输出，形状 (batch_size, seq_len_q, d_model)。\n",
        "        attention_weights: 所有头的注意力权重 (用于可视化或调试)，形状 (batch_size, num_heads, seq_len_q, seq_len_k)。\n",
        "    \"\"\"\n",
        "    depth = d_model // num_heads\n",
        "    assert d_model % num_heads == 0 # d_model 必须能被 num_heads 整除\n",
        "\n",
        "    # 1. 通过线性层进行 Q, K, V 的投影\n",
        "    # 形状 (batch_size, seq_len, d_model)\n",
        "    q_proj = np.matmul(q, Wq)\n",
        "    k_proj = np.matmul(k, Wk)\n",
        "    v_proj = np.matmul(v, Wv)\n",
        "\n",
        "    # 2. 分割成 num_heads\n",
        "    # 形状 (batch_size, num_heads, seq_len, depth)\n",
        "    q_heads = split_heads(q_proj, num_heads, depth)\n",
        "    k_heads = split_heads(k_proj, num_heads, depth)\n",
        "    v_heads = split_heads(v_proj, num_heads, depth)\n",
        "\n",
        "    # 3. 对每个头进行缩放点积注意力\n",
        "    # attention_output 形状 (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights 形状 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    attention_output, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask)\n",
        "\n",
        "    # 4. 合并所有头的输出\n",
        "    # 形状 (batch_size, seq_len_q, d_model)\n",
        "    output_combined = combine_heads(attention_output)\n",
        "\n",
        "    # 5. 最终的线性投影\n",
        "    # 形状 (batch_size, seq_len_q, d_model)\n",
        "    output = np.matmul(output_combined, Wo)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# --- Test Multi-Head Attention ---\n",
        "print(\"--- Test Multi-Head Attention ---\")\n",
        "batch_size = 2\n",
        "seq_len_q = 4\n",
        "seq_len_k = 5 # Key/Value sequence can be different length (e.g., encoder output)\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "\n",
        "# Initialize random weights (in a real model, these would be learned)\n",
        "# For simplicity, we use d_model for both input and output dimensions of linear layers\n",
        "Wq_test = np.random.rand(d_model, d_model)\n",
        "Wk_test = np.random.rand(d_model, d_model)\n",
        "Wv_test = np.random.rand(d_model, d_model)\n",
        "Wo_test = np.random.rand(d_model, d_model)\n",
        "\n",
        "# Simulate input tensors\n",
        "q_test = np.random.rand(batch_size, seq_len_q, d_model)\n",
        "k_test = np.random.rand(batch_size, seq_len_k, d_model)\n",
        "v_test = np.random.rand(batch_size, seq_len_k, d_model)\n",
        "\n",
        "# Create a simple padding mask (e.g., last token in key/value is padding)\n",
        "# mask_test = np.zeros((batch_size, 1, 1, seq_len_k))\n",
        "# mask_test[:, :, :, -1] = 1 # Mask the last position\n",
        "mask_test = None # No mask for this test\n",
        "\n",
        "mha_output, mha_weights = multi_head_attention(q_test, k_test, v_test,\n",
        "                          mask_test, d_model, num_heads,\n",
        "                          Wq_test, Wk_test, Wv_test, Wo_test)\n",
        "\n",
        "print(f\"Input Q shape: {q_test.shape}\")\n",
        "print(f\"Input K shape: {k_test.shape}\")\n",
        "print(f\"Input V shape: {v_test.shape}\")\n",
        "print(f\"Multi-Head Attention Output shape: {mha_output.shape}\") # Expected: (2, 4, 64)\n",
        "print(f\"Multi-Head Attention Weights shape: {mha_weights.shape}\") # Expected: (2, 8, 4, 5)\n",
        "print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Multi-Head Attention ---\n",
            "Input Q shape: (2, 4, 64)\n",
            "Input K shape: (2, 5, 64)\n",
            "Input V shape: (2, 5, 64)\n",
            "Multi-Head Attention Output shape: (2, 4, 64)\n",
            "Multi-Head Attention Weights shape: (2, 8, 4, 5)\n",
            "--------------------\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv3nB0Gp8P_w",
        "outputId": "0a6b4640-91d3-401f-e839-73e874a62160"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Feed-Forward Network (前馈网络)**\n",
        "\n",
        "一个简单的两层网络，中间使用 ReLU 激活函数。\n",
        "\n",
        "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$"
      ],
      "metadata": {
        "id": "Zd37oEFK8P_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(x, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "    简单的两层前馈网络。\n",
        "\n",
        "    Args:\n",
        "        x: 输入张量，形状 (batch_size, seq_len, d_model)。\n",
        "        W1: 第一层权重，形状 (d_model, d_ff)。\n",
        "        b1: 第一层偏置，形状 (d_ff,)。\n",
        "        W2: 第二层权重，形状 (d_ff, d_model)。\n",
        "        b2: 第二层偏置，形状 (d_model,)。\n",
        "\n",
        "    Returns:\n",
        "        输出张量，形状 (batch_size, seq_len, d_model)。\n",
        "    \"\"\"\n",
        "    # 第一层线性变换 + ReLU\n",
        "    # 形状 (batch_size, seq_len, d_ff)\n",
        "    hidden = np.maximum(0, np.matmul(x, W1) + b1)\n",
        "\n",
        "    # 第二层线性变换\n",
        "    # 形状 (batch_size, seq_len, d_model)\n",
        "    output = np.matmul(hidden, W2) + b2\n",
        "\n",
        "    return output\n",
        "\n",
        "# --- Test Feed-Forward Network ---\n",
        "print(\"--- Test Feed-Forward Network ---\")\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "d_model = 128\n",
        "d_ff = 512 # Inner dimension\n",
        "\n",
        "# Initialize random parameters\n",
        "W1_test_ffn = np.random.rand(d_model, d_ff)\n",
        "b1_test_ffn = np.random.rand(d_ff)\n",
        "W2_test_ffn = np.random.rand(d_ff, d_model)\n",
        "b2_test_ffn = np.random.rand(d_model)\n",
        "\n",
        "# Simulate input\n",
        "input_test_ffn = np.random.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "ffn_output_test = feed_forward_network(input_test_ffn,\n",
        "                     W1_test_ffn, b1_test_ffn,\n",
        "                     W2_test_ffn, b2_test_ffn)\n",
        "\n",
        "print(f\"FFN Input shape: {input_test_ffn.shape}\")\n",
        "print(f\"FFN Output shape: {ffn_output_test.shape}\") # Expected: (2, 10, 128)\n",
        "print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Feed-Forward Network ---\n",
            "FFN Input shape: (2, 10, 128)\n",
            "FFN Output shape: (2, 10, 128)\n",
            "--------------------\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7b8e3Co8P_0",
        "outputId": "24c641dd-04d4-4ff5-8d2c-f8ffc49f26b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Add & Norm (残差连接与层归一化)**\n",
        "\n",
        "包含两部分：\n",
        "1.  **残差连接:** 将子层的输入直接加到其输出上：$x + \\text{Sublayer}(x)$。\n",
        "2.  **层归一化:** 对特征维度进行归一化。\n",
        "\n",
        "$$LN(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
        "\n",
        "其中 $\\mu$ 和 $\\sigma^2$ 是输入 $x$ 在最后一个维度（特征维度）上的均值和方差，$\\gamma$ 和 $\\beta$ 是可学习的缩放和偏移参数，$\\epsilon$ 是为数值稳定性添加的小数。"
      ],
      "metadata": {
        "id": "6TzLhn2M8P_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(x, gamma, beta, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    执行层归一化。\n",
        "\n",
        "    Args:\n",
        "        x: 输入张量。\n",
        "        gamma: 可学习的缩放参数，形状与 x 的最后一个维度相同。\n",
        "        beta: 可学习的偏移参数，形状与 x 的最后一个维度相同。\n",
        "        epsilon: 用于数值稳定性的小值。\n",
        "\n",
        "    Returns:\n",
        "        归一化后的张量，形状与 x 相同。\n",
        "    \"\"\"\n",
        "    # 计算最后一个维度上的均值和方差\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    var = np.var(x, axis=-1, keepdims=True)\n",
        "\n",
        "    # 归一化\n",
        "    normalized_x = (x - mean) / np.sqrt(var + epsilon)\n",
        "\n",
        "    # 缩放和偏移\n",
        "    output = gamma * normalized_x + beta\n",
        "    return output\n",
        "\n",
        "def add_and_norm(x, sublayer_output, gamma, beta):\n",
        "    \"\"\"\n",
        "    执行残差连接和层归一化。\n",
        "\n",
        "    Args:\n",
        "        x: 子层的输入张量。\n",
        "        sublayer_output: 子层的输出张量 (MHA 或 FFN)。\n",
        "        gamma: 层归一化的 gamma 参数。\n",
        "        beta: 层归一化的 beta 参数。\n",
        "\n",
        "    Returns:\n",
        "        经过残差连接和归一化后的张量。\n",
        "    \"\"\"\n",
        "    # 残差连接\n",
        "    added = x + sublayer_output\n",
        "    # 层归一化\n",
        "    normed = layer_norm(added, gamma, beta)\n",
        "    return normed\n",
        "\n",
        "# --- Test Add & Norm ---\n",
        "print(\"--- Test Add & Norm ---\")\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 32\n",
        "\n",
        "# Simulate input and sublayer output\n",
        "input_test_an = np.random.rand(batch_size, seq_len, d_model)\n",
        "sublayer_output_test_an = np.random.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# Initialize random gamma and beta for LayerNorm\n",
        "gamma_test_an = np.random.rand(d_model)\n",
        "beta_test_an = np.random.rand(d_model)\n",
        "\n",
        "\n",
        "add_norm_output_test = add_and_norm(input_test_an, sublayer_output_test_an,\n",
        "                    gamma_test_an, beta_test_an)\n",
        "\n",
        "print(f\"Input shape for Add & Norm: {input_test_an.shape}\")\n",
        "print(f\"Sublayer output shape for Add & Norm: {sublayer_output_test_an.shape}\")\n",
        "print(f\"Add & Norm Output shape: {add_norm_output_test.shape}\")\n",
        "# Expected: (2, 5, 32)\n",
        "# Check if mean and variance of the last dimension are close to 0 and 1 after normalization (before gamma/beta)\n",
        "# For simplicity, skip detailed numerical check here, rely on shape check.\n",
        "print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Add & Norm ---\n",
            "Input shape for Add & Norm: (2, 5, 32)\n",
            "Sublayer output shape for Add & Norm: (2, 5, 32)\n",
            "Add & Norm Output shape: (2, 5, 32)\n",
            "--------------------\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3huslBSw8P_1",
        "outputId": "a44be870-d734-4a84-e285-93ff54a5fbd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Encoder Layer (编码器层)**\n",
        "\n",
        "一个标准的编码器层包含一个多头自注意力模块，接着是 Add & Norm，然后是一个前馈网络，最后再是一个 Add & Norm。"
      ],
      "metadata": {
        "id": "vHC_UvT58P_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(x, mask, d_model, num_heads, d_ff,\n",
        "          # MHA 参数\n",
        "          Wq_mha, Wk_mha, Wv_mha, Wo_mha, gamma1_ln, beta1_ln,\n",
        "          # FFN 参数\n",
        "          W1_ffn, b1_ffn, W2_ffn, b2_ffn, gamma2_ln, beta2_ln):\n",
        "    \"\"\"\n",
        "    实现一个 Transformer 编码器层。\n",
        "\n",
        "    Args:\n",
        "        x: 输入张量，形状 (batch_size, seq_len, d_model)。\n",
        "        mask: 自注意力模块的 mask 张量，形状 (batch_size, 1, seq_len, seq_len)。\n",
        "        d_model: 模型的维度。\n",
        "        num_heads: 多头注意力的头数。\n",
        "        d_ff: 前馈网络的内部维度。\n",
        "        ... 其他参数为各子层的权重、偏置、gamma、beta。\n",
        "\n",
        "    Returns:\n",
        "        输出张量，形状 (batch_size, seq_len, d_model)。\n",
        "    \"\"\"\n",
        "\n",
        "    # Layer 1: Multi-Head Self Attention + Add & Norm\n",
        "    # 在自注意力中，Q, K, V 都来自同一个输入 x\n",
        "    attn_output, _ = multi_head_attention(\n",
        "        x, x, x, mask, d_model, num_heads,\n",
        "        Wq_mha, Wk_mha, Wv_mha, Wo_mha\n",
        "    )\n",
        "    # 残差连接和层归一化\n",
        "    out1 = add_and_norm(x, attn_output, gamma1_ln, beta1_ln)\n",
        "\n",
        "    # Layer 2: Feed Forward + Add & Norm\n",
        "    ffn_output = feed_forward_network(\n",
        "        out1, W1_ffn, b1_ffn, W2_ffn, b2_ffn\n",
        "    )\n",
        "    # 残差连接和层归一化\n",
        "    out2 = add_and_norm(out1, ffn_output, gamma2_ln, beta2_ln)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# # --- Test Encoder Layer ---\n",
        "# print(\"--- Test Encoder Layer ---\")\n",
        "# batch_size = 2\n",
        "# seq_len = 10\n",
        "# d_model = 128\n",
        "# num_heads = 8\n",
        "# d_ff = 512\n",
        "\n",
        "# # Simulate input\n",
        "# input_test_el = np.random.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# # Create a dummy mask (e.g., for padding or no mask for encoder)\n",
        "# # In a real encoder, the mask would hide padding tokens.\n",
        "# # Here, we use None for a simple test (no masking) or create a dummy padding mask.\n",
        "# # Example padding mask: assume the last 2 tokens are padding in batch 0, and last 1 in batch 1\n",
        "# mask_test_el = np.zeros((batch_size, 1, seq_len, seq_len))\n",
        "# # For batch 0, mask positions 8 and 9\n",
        "# # mask_test_el[0, :, :, 8:] = 1 # This mask is applied to K/V dimensions, so mask columns\n",
        "# # mask_test_el[0, :, 8:, :] = 1 # If it's self attention, Q dimension is also masked, mask rows\n",
        "\n",
        "# # A simpler encoder self-attention mask just handles padding.\n",
        "# # Let's assume batch 0 has seq_len 8, batch 1 has seq_len 9\n",
        "# seq_lens = [8, 9]\n",
        "# mask_test_el = np.array([[ [ [0 if i < seq_len else 1 for i in range(seq_len)] for _ in range(seq_len)] for _ in range(1)] for seq_len in seq_lens])\n",
        "# mask_test_el = mask_test_el[:, np.newaxis, ...] # Add head dimension placeholder (batch, 1, 1, seq_len)\n",
        "\n",
        "# print(f\"Mask shape for Encoder Layer: {mask_test_el.shape}\")\n",
        "\n",
        "# # Initialize ALL parameters for the layer randomly\n",
        "# Wq_mha_test = np.random.rand(d_model, d_model)\n",
        "# Wk_mha_test = np.random.rand(d_model, d_model)\n",
        "# Wv_mha_test = np.random.rand(d_model, d_model)\n",
        "# Wo_mha_test = np.random.rand(d_model, d_model)\n",
        "# gamma1_ln_test = np.random.rand(d_model)\n",
        "# beta1_ln_test = np.random.rand(d_model)\n",
        "\n",
        "# W1_ffn_test = np.random.rand(d_model, d_ff)\n",
        "# b1_ffn_test = np.random.rand(d_ff)\n",
        "# W2_ffn_test = np.random.rand(d_ff, d_model)\n",
        "# b2_ffn_test = np.random.rand(d_model)\n",
        "# gamma2_ln_test = np.random.rand(d_model)\n",
        "# beta2_ln_test = np.random.rand(d_model)\n",
        "\n",
        "# encoder_output_test = encoder_layer(\n",
        "#     input_test_el, mask_test_el, d_model, num_heads, d_ff,\n",
        "#     Wq_mha_test, Wk_mha_test, Wv_mha_test, Wo_mha_test, gamma1_ln_test, beta1_ln_test,\n",
        "#     W1_ffn_test, b1_ffn_test, W2_ffn_test, b2_ffn_test, gamma2_ln_test, beta2_ln_test\n",
        "# )\n",
        "\n",
        "# print(f\"Encoder Layer Input shape: {input_test_el.shape}\")\n",
        "# print(f\"Encoder Layer Output shape: {encoder_output_test.shape}\") # Expected: (2, 10, 128)\n",
        "# print(\"-\" * 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Encoder Layer ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 1) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-201c6a7df990>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Let's assume batch 0 has seq_len 8, batch 1 has seq_len 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mseq_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmask_test_el\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mmask_test_el\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_test_el\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Add head dimension placeholder (batch, 1, 1, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 1) + inhomogeneous part."
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "44_xsh1J8P_2",
        "outputId": "516e9f12-0a3d-4903-b897-97a320ebb3eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Encoder Layer ---\n",
        "print(\"--- Test Encoder Layer ---\")\n",
        "batch_size = 2\n",
        "seq_len = 10  # 批次中的最大序列长度 (tensor 的实际长度)\n",
        "num_heads = 8\n",
        "d_model = 128\n",
        "d_ff = 512\n",
        "\n",
        "# Simulate input\n",
        "input_test_el = np.random.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# Create a dummy mask for padding based on actual sequence lengths\n",
        "# Assume actual sequence lengths for items in the batch are different\n",
        "actual_seq_lens = [8, 9] # 第一个序列实际长度为8，第二个为9\n",
        "\n",
        "# Create a mask of zeros with the target shape (batch, 1, 1, seq_len)\n",
        "# The 1s are for broadcasting across heads and the query sequence length dimension\n",
        "mask_test_el = np.zeros((batch_size, 1, 1, seq_len))\n",
        "\n",
        "# Set mask values to 1 for positions beyond the actual sequence length\n",
        "for i, current_seq_len in enumerate(actual_seq_lens):\n",
        "    if current_seq_len < seq_len:\n",
        "        # 对于当前批次项，从实际长度之后的位置到最大长度都设置为 1\n",
        "        mask_test_el[i, :, :, current_seq_len:] = 1\n",
        "\n",
        "# mask_test_el 的形状现在是 (2, 1, 1, 10)，可以正确广播到注意力分数 (2, 8, 10, 10)\n",
        "print(f\"Mask shape for Encoder Layer: {mask_test_el.shape}\")\n",
        "# 打印一个例子看看掩码内容\n",
        "print(\"Sample Mask for batch item 0:\\n\", mask_test_el[0, 0, 0, :])\n",
        "print(\"Sample Mask for batch item 1:\\n\", mask_test_el[1, 0, 0, :])\n",
        "\n",
        "\n",
        "# Initialize ALL parameters for the layer randomly (unchanged)\n",
        "Wq_mha_test = np.random.rand(d_model, d_model)\n",
        "Wk_mha_test = np.random.rand(d_model, d_model)\n",
        "Wv_mha_test = np.random.rand(d_model, d_model)\n",
        "Wo_mha_test = np.random.rand(d_model, d_model)\n",
        "gamma1_ln_test = np.random.rand(d_model)\n",
        "beta1_ln_test = np.random.rand(d_model)\n",
        "\n",
        "W1_ffn_test = np.random.rand(d_model, d_ff)\n",
        "b1_ffn_test = np.random.rand(d_ff)\n",
        "W2_ffn_test = np.random.rand(d_ff, d_model)\n",
        "b2_ffn_test = np.random.rand(d_model)\n",
        "gamma2_ln_test = np.random.rand(d_model)\n",
        "beta2_ln_test = np.random.rand(d_model)\n",
        "\n",
        "encoder_output_test = encoder_layer(\n",
        "    input_test_el, mask_test_el, d_model, num_heads, d_ff,\n",
        "    Wq_mha_test, Wk_mha_test, Wv_mha_test, Wo_mha_test, gamma1_ln_test, beta1_ln_test,\n",
        "    W1_ffn_test, b1_ffn_test, W2_ffn_test, b2_ffn_test, gamma2_ln_test, beta2_ln_test\n",
        ")\n",
        "\n",
        "print(f\"Encoder Layer Input shape: {input_test_el.shape}\")\n",
        "print(f\"Encoder Layer Output shape: {encoder_output_test.shape}\") # Expected: (2, 10, 128)\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M4OVdrhB48w",
        "outputId": "52218643-5a5a-4b5c-f25a-3afefd4bd609"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Encoder Layer ---\n",
            "Mask shape for Encoder Layer: (2, 1, 1, 10)\n",
            "Sample Mask for batch item 0:\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample Mask for batch item 1:\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Encoder Layer Input shape: (2, 10, 128)\n",
            "Encoder Layer Output shape: (2, 10, 128)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Components (简要说明)**\n",
        "\n",
        "解码器层与编码器层类似，但有几个关键区别：\n",
        "\n",
        "1.  **Masked Multi-Head Self-Attention:** 在解码器的第一个多头自注意力层中，需要使用 *Look-ahead mask* 来防止注意力关注到未来位置的 token，从而保持自回归属性。\n",
        "2.  **Cross-Attention:** 解码器还有一个多头注意力层，称为交叉注意力。其 Query 来自于前一个解码器层的输出，而 Key 和 Value 来自于 *编码器的输出*。这里通常也需要 Padding mask 来屏蔽编码器输出中的 padding。\n",
        "3.  解码器层结构：Masked MSA -> Add & Norm -> Cross-Attention -> Add & Norm -> FFN -> Add & Norm。\n",
        "\n",
        "为了保持代码的简洁性，我们只详细复现了编码器部分的组件，因为它们包含了 Transformer 的核心机制。实现解码器组件会在 `scaled_dot_product_attention` 中使用不同的 mask（Look-ahead mask for self-attention, Padding mask for cross-attention）并调整 `multi_head_attention` 的输入 Q, K, V 的来源即可。\n",
        "\n",
        "**测试说明:**\n",
        "\n",
        "上面的代码块中，每个组件实现之后都紧跟着一个简单的测试部分。这些测试：\n",
        "1.  定义了模拟的输入张量形状和参数。\n",
        "2.  使用 `np.random.rand` 或 `np.zeros` 初始化随机或零参数。\n",
        "3.  调用相应的函数。\n",
        "4.  打印输入和输出的形状，以验证尺寸是否匹配预期。\n",
        "\n",
        "这些测试主要用于验证算法的维度正确性，而不是验证计算结果的数值正确性（因为参数是随机的）。在一个真正的框架中，会编写更详细的单元测试来检查数值计算是否符合预期。\n",
        "\n",
        "这个简单的 NumPy 实现展示了 Transformer 各个组件的底层数学运算。在深度学习框架（如 PyTorch, TensorFlow）中，这些操作会被高度优化，并且参数的管理（初始化、更新）由框架自动处理。"
      ],
      "metadata": {
        "id": "fqph1WiD8P_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decoder_layer(x, encoder_output, lookahead_mask, padding_mask, d_model, num_heads, d_ff,\n",
        "                  # Masked MSA parameters\n",
        "                  Wq_msa, Wk_msa, Wv_msa, Wo_msa, gamma1_ln, beta1_ln,\n",
        "                  # Cross-Attention parameters\n",
        "                  Wq_ca, Wk_ca, Wv_ca, Wo_ca, gamma2_ln, beta2_ln,\n",
        "                  # FFN parameters\n",
        "                  W1_ffn, b1_ffn, W2_ffn, b2_ffn, gamma3_ln, beta3_ln):\n",
        "    \"\"\"\n",
        "    Implements a Transformer decoder layer.\n",
        "    \"\"\"\n",
        "    # 1. Masked Multi-Head Self-Attention\n",
        "    masked_msa_output, _ = multi_head_attention(\n",
        "        x, x, x, lookahead_mask, d_model, num_heads, Wq_msa, Wk_msa, Wv_msa, Wo_msa\n",
        "    )\n",
        "    out1 = add_and_norm(x, masked_msa_output, gamma1_ln, beta1_ln)\n",
        "\n",
        "    # 2. Cross-Attention\n",
        "    cross_attn_output, _ = multi_head_attention(\n",
        "        out1, encoder_output, encoder_output, padding_mask, d_model, num_heads,\n",
        "        Wq_ca, Wk_ca, Wv_ca, Wo_ca\n",
        "    )\n",
        "    out2 = add_and_norm(out1, cross_attn_output, gamma2_ln, beta2_ln)\n",
        "\n",
        "    # 3. Feed Forward Network\n",
        "    ffn_output = feed_forward_network(out2, W1_ffn, b1_ffn, W2_ffn, b2_ffn)\n",
        "    out3 = add_and_norm(out2, ffn_output, gamma3_ln, beta3_ln)\n",
        "\n",
        "    return out3\n",
        "\n",
        "\n",
        "# --- Test Decoder Layer ---\n",
        "print(\"--- Test Decoder Layer ---\")\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 512\n",
        "\n",
        "# Simulate inputs\n",
        "x = np.random.rand(batch_size, seq_len, d_model)\n",
        "encoder_output = np.random.rand(batch_size, seq_len, d_model)\n",
        "\n",
        "# Create dummy masks\n",
        "lookahead_mask = np.zeros((batch_size, 1, seq_len, seq_len))  # Example, needs adjustment\n",
        "padding_mask = np.zeros((batch_size, 1, 1, seq_len)) # Example, needs adjustment\n",
        "\n",
        "\n",
        "# Initialize parameters (replace with actual weight initialization)\n",
        "# ... all weight parameters for MSA, cross-attention, FFN, layer normalization\n",
        "\n",
        "Wq_msa = np.random.rand(d_model, d_model)\n",
        "Wk_msa = np.random.rand(d_model, d_model)\n",
        "Wv_msa = np.random.rand(d_model, d_model)\n",
        "Wo_msa = np.random.rand(d_model, d_model)\n",
        "gamma1_ln = np.random.rand(d_model)\n",
        "beta1_ln = np.random.rand(d_model)\n",
        "\n",
        "Wq_ca = np.random.rand(d_model, d_model)\n",
        "Wk_ca = np.random.rand(d_model, d_model)\n",
        "Wv_ca = np.random.rand(d_model, d_model)\n",
        "Wo_ca = np.random.rand(d_model, d_model)\n",
        "gamma2_ln = np.random.rand(d_model)\n",
        "beta2_ln = np.random.rand(d_model)\n",
        "\n",
        "W1_ffn = np.random.rand(d_model, d_ff)\n",
        "b1_ffn = np.random.rand(d_ff)\n",
        "W2_ffn = np.random.rand(d_ff, d_model)\n",
        "b2_ffn = np.random.rand(d_model)\n",
        "gamma3_ln = np.random.rand(d_model)\n",
        "beta3_ln = np.random.rand(d_model)\n",
        "\n",
        "\n",
        "decoder_output = decoder_layer(\n",
        "    x, encoder_output, lookahead_mask, padding_mask, d_model, num_heads, d_ff,\n",
        "    Wq_msa, Wk_msa, Wv_msa, Wo_msa, gamma1_ln, beta1_ln,\n",
        "    Wq_ca, Wk_ca, Wv_ca, Wo_ca, gamma2_ln, beta2_ln,\n",
        "    W1_ffn, b1_ffn, W2_ffn, b2_ffn, gamma3_ln, beta3_ln\n",
        ")\n",
        "\n",
        "print(f\"Decoder Layer Output shape: {decoder_output.shape}\")  # Expected (2,10,128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yv_sB5oCIN6",
        "outputId": "b246df3a-f081-43c0-9a64-991f7db0b4f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Decoder Layer ---\n",
            "Decoder Layer Output shape: (2, 10, 128)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}